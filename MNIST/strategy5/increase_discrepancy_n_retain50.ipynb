{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.autograd as autograd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../weights/mnist_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = [module for module in model.modules() if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear)]\n",
    "\n",
    "module_shape = [m.weight.shape for m in module_list]\n",
    "\n",
    "original_weights = [m.weight for m in module_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetSubnet(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, scores, k):\n",
    "        # Get the supermask by sorting the scores and using the top k%\n",
    "        out = scores.clone()\n",
    "        _, idx = scores.flatten().sort()\n",
    "        j = int((1 - k) * scores.numel())\n",
    "\n",
    "        # flat_out and out access the same memory.\n",
    "        flat_out = out.flatten()\n",
    "        flat_out[idx[:j]] = 0\n",
    "        flat_out[idx[j:]] = 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        return g, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupermaskConv(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # initialize the scores\n",
    "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
    "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
    "\n",
    "        # NOTE: initialize the weights like this.\n",
    "        nn.init.kaiming_normal_(self.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "        # NOTE: turn the gradient on the weights off\n",
    "        self.weight.requires_grad = False\n",
    "        self.scores.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        subnet = GetSubnet.apply(self.scores.abs(), sparsity)\n",
    "        w = self.weight * subnet\n",
    "        x = F.conv2d(\n",
    "            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "        )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupermaskLinear(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # initialize the scores\n",
    "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
    "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
    "\n",
    "        # NOTE: initialize the weights like this.\n",
    "        nn.init.kaiming_normal_(self.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "        # NOTE: turn the gradient on the weights off\n",
    "        self.weight.requires_grad = False\n",
    "        self.scores.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        subnet = GetSubnet.apply(self.scores.abs(), sparsity)\n",
    "        w = self.weight * subnet\n",
    "        \n",
    "        return F.linear(x, w, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANet, self).__init__()\n",
    "        self.conv1 = SupermaskConv(1, 32, 3, 1, bias=False)\n",
    "        self.conv2 = SupermaskConv(32, 64, 3, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = SupermaskLinear(9216, 128, bias=False)\n",
    "        self.fc2 = SupermaskLinear(128, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        os.path.join(\"../data\", \"mnist\"),\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True\n",
    "    ),\n",
    "    batch_size=10000,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.5\n",
    "seed = 1507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in test_loader:\n",
    "    test_data, test_target = data.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(test_data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct = pred.eq(test_target.view_as(pred)).sum().item()\n",
    "        out = correct / len(test_data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_model = GANet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "ga_module_list = [module for module in ga_model.modules() if isinstance(module, SupermaskConv) or isinstance(module, SupermaskLinear)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weight of ga model with original trained model \n",
    "\n",
    "for i, weight in enumerate(original_weights):\n",
    "    ga_module_list[i].weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.fitness = 0\n",
    "        \n",
    "    def set_fitness(self, fitness):\n",
    "        self.fitness = fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init population\n",
    "\n",
    "def init_pop(pop_size=100):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        params = []\n",
    "        for shape in module_shape:\n",
    "            scores = nn.Parameter(torch.Tensor(shape))\n",
    "            nn.init.kaiming_uniform_(scores, a=math.sqrt(5))\n",
    "            params.append(scores)\n",
    "        agent = Agent(params=params)\n",
    "        population.append(agent)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_scores(module_list, agent):\n",
    "    for i, m_scores in enumerate(agent.params):\n",
    "        module_list[i].scores = m_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(agent, mut_rate=0.1):\n",
    "    params = []\n",
    "    for param in agent.params:\n",
    "        out = param.clone()\n",
    "        # flat_out and out share the same memory\n",
    "        flat_out = out.flatten().to(device)\n",
    "        # Get index mutation \n",
    "        indexes = np.where(np.random.uniform(low=0, high=1, size=(len(flat_out))) < mut_rate)\n",
    "        replace_values = np.random.uniform(low=-1, high=1, size=(len(flat_out)))[indexes]\n",
    "        # Mutation\n",
    "        flat_out.index_copy_(0, torch.LongTensor(indexes[0]).to(device), torch.FloatTensor(replace_values).to(device))\n",
    "        params.append(nn.Parameter(out))\n",
    "    return Agent(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombine_agent(agent_1, agent_2):\n",
    "    params_1 = []\n",
    "    params_2 = []\n",
    "    for i, param in enumerate(agent_1.params):\n",
    "        param_1 = param.clone()\n",
    "        param_2 = agent_2.params[i].clone()\n",
    "        # Flatten \n",
    "        flat_1 = param_1.flatten().to(device)\n",
    "        flat_2 = param_2.flatten().to(device)\n",
    "        # Define children\n",
    "        child_1 = torch.zeros(len(flat_1))\n",
    "        child_2 = torch.zeros(len(flat_1))\n",
    "        # Select cross point\n",
    "        cross_pt = random.randint(0, len(flat_1))\n",
    "        # Swap\n",
    "        child_1[cross_pt:len(flat_1)] = flat_1[cross_pt:len(flat_1)]\n",
    "        child_1[0:cross_pt] = flat_2[0:cross_pt]\n",
    "        child_2[cross_pt:len(flat_1)] = flat_2[cross_pt:len(flat_1)]\n",
    "        child_2[0:cross_pt] = flat_1[0:cross_pt]\n",
    "        # Append to params \n",
    "        params_1.append(nn.Parameter(child_1.reshape(module_shape[i])))\n",
    "        params_2.append(nn.Parameter(child_2.reshape(module_shape[i])))\n",
    "\n",
    "    return Agent(params_1), Agent(params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def evaluate_population(pop, gen):\n",
    "    avg_fit = 0\n",
    "    best_fit = 0\n",
    "    for agent in tqdm(pop):\n",
    "        change_scores(ga_module_list, agent)\n",
    "        fit = test(ga_model.to(device), device)\n",
    "        agent.fitness = fit\n",
    "        avg_fit += fit\n",
    "        if fit > best_fit:\n",
    "            best_fit = fit\n",
    "    avg_fit /= len(pop)\n",
    "    return pop, avg_fit, best_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_generation(pop, size=100, mut_rate=0.01):\n",
    "    # retain top 50% of current generation\n",
    "    pop.sort(key=lambda x: x.fitness, reverse=True)\n",
    "    n_retain = int(0.5 * len(pop)) # number of individuals retained\n",
    "    new_pop = pop[:n_retain]\n",
    "    while len(new_pop) < size:\n",
    "        parents = random.choices(pop, k=2, weights=[x.fitness**2 for x in pop])\n",
    "        offspring_ = recombine_agent(parents[0],parents[1])\n",
    "        offspring = [mutation(offspring_[0], mut_rate=mut_rate), mutation(offspring_[1], mut_rate=mut_rate)]\n",
    "        new_pop.extend(offspring) #add offspring to next generation\n",
    "    return new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 300\n",
    "population_size = 50\n",
    "\n",
    "pop = init_pop(population_size)\n",
    "\n",
    "mutation_rate = 0.15 # 0.1% mutation rate\n",
    "\n",
    "pop_fit = []\n",
    "\n",
    "pop = init_pop(population_size) # initial population\n",
    "\n",
    "for gen in range(num_generations):\n",
    "    # trainning\n",
    "    pop, avg_fit, best_fit = evaluate_population(pop, gen)\n",
    "    if avg_fit > 0.8:\n",
    "        population_size = 100\n",
    "        mutation_rate = 0.1\n",
    "    if avg_fit > 0.85:\n",
    "        population_size = 150\n",
    "        mutation_rate = 0.05\n",
    "    if avg_fit > 0.9:\n",
    "        population_size = 300\n",
    "        mutation_rate = 0.025\n",
    "    if avg_fit > 0.95:\n",
    "        population_size = 600\n",
    "        mutation_rate = 0.01\n",
    "    print('Generation {} with pop_fit {} | best_fit {}'.format(gen, avg_fit, best_fit))\n",
    "    pop_fit.append(avg_fit) # record population average fitness\n",
    "    new_pop = next_generation(pop, size=population_size, mut_rate=mutation_rate)\n",
    "    pop = new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d026963d5416c1b0f433f3cabaad95ff01da743e2188d1b128ee6199f35f8583"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
