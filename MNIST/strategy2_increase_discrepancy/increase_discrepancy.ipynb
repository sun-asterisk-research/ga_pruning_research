{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../weights/mnist_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = [module for module in model.modules() if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear)]\n",
    "\n",
    "module_shape = [m.weight.shape for m in module_list]\n",
    "\n",
    "original_weights = [m.weight for m in module_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetSubnet(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, scores, k):\n",
    "        # Get the supermask by sorting the scores and using the top k%\n",
    "        out = scores.clone()\n",
    "        _, idx = scores.flatten().sort()\n",
    "        j = int((1 - k) * scores.numel())\n",
    "\n",
    "        # flat_out and out access the same memory.\n",
    "        flat_out = out.flatten()\n",
    "        flat_out[idx[:j]] = 0\n",
    "        flat_out[idx[j:]] = 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        return g, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupermaskConv(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # initialize the scores\n",
    "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
    "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
    "\n",
    "        # NOTE: initialize the weights like this.\n",
    "        nn.init.kaiming_normal_(self.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "        # NOTE: turn the gradient on the weights off\n",
    "        self.weight.requires_grad = False\n",
    "        self.scores.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        subnet = GetSubnet.apply(self.scores.abs(), sparsity)\n",
    "        w = self.weight * subnet\n",
    "        x = F.conv2d(\n",
    "            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "        )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupermaskLinear(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # initialize the scores\n",
    "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
    "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
    "\n",
    "        # NOTE: initialize the weights like this.\n",
    "        nn.init.kaiming_normal_(self.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "        # NOTE: turn the gradient on the weights off\n",
    "        self.weight.requires_grad = False\n",
    "        self.scores.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        subnet = GetSubnet.apply(self.scores.abs(), sparsity)\n",
    "        w = self.weight * subnet\n",
    "        \n",
    "        return F.linear(x, w, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANet, self).__init__()\n",
    "        self.conv1 = SupermaskConv(1, 32, 3, 1, bias=False)\n",
    "        self.conv2 = SupermaskConv(32, 64, 3, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = SupermaskLinear(9216, 128, bias=False)\n",
    "        self.fc2 = SupermaskLinear(128, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen.tung.thanh/anaconda3/envs/fr/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        os.path.join(\"../data\", \"mnist\"),\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True\n",
    "    ),\n",
    "    batch_size=10000,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.5\n",
    "seed = 1507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcfe00e9610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in test_loader:\n",
    "    test_data, test_target = data.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(test_data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct = pred.eq(test_target.view_as(pred)).sum().item()\n",
    "        out = correct / len(test_data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_model = GANet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "ga_module_list = [module for module in ga_model.modules() if isinstance(module, SupermaskConv) or isinstance(module, SupermaskLinear)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weight of ga model with original trained model \n",
    "\n",
    "for i, weight in enumerate(original_weights):\n",
    "    ga_module_list[i].weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.fitness = 0\n",
    "        \n",
    "    def set_fitness(self, fitness):\n",
    "        self.fitness = fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init population\n",
    "\n",
    "def init_pop(pop_size=100):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        params = []\n",
    "        for shape in module_shape:\n",
    "            scores = nn.Parameter(torch.Tensor(shape))\n",
    "            nn.init.kaiming_uniform_(scores, a=math.sqrt(5))\n",
    "            params.append(scores)\n",
    "        agent = Agent(params=params)\n",
    "        population.append(agent)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_scores(module_list, agent):\n",
    "    for i, m_scores in enumerate(agent.params):\n",
    "        module_list[i].scores = m_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(agent, mut_rate=0.1):\n",
    "    params = []\n",
    "    for param in agent.params:\n",
    "        out = param.clone()\n",
    "        # flat_out and out share the same memory\n",
    "        flat_out = out.flatten().to(device)\n",
    "        # Get index mutation \n",
    "        indexes = np.where(np.random.uniform(low=0, high=1, size=(len(flat_out))) < mut_rate)\n",
    "        replace_values = np.random.uniform(low=-1, high=1, size=(len(flat_out)))[indexes]\n",
    "        # Mutation\n",
    "        flat_out.index_copy_(0, torch.LongTensor(indexes[0]).to(device), torch.FloatTensor(replace_values).to(device))\n",
    "        params.append(nn.Parameter(out))\n",
    "    return Agent(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombine_agent(agent_1, agent_2):\n",
    "    params_1 = []\n",
    "    params_2 = []\n",
    "    for i, param in enumerate(agent_1.params):\n",
    "        param_1 = param.clone()\n",
    "        param_2 = agent_2.params[i].clone()\n",
    "        # Flatten \n",
    "        flat_1 = param_1.flatten().to(device)\n",
    "        flat_2 = param_2.flatten().to(device)\n",
    "        # Define children\n",
    "        child_1 = torch.zeros(len(flat_1))\n",
    "        child_2 = torch.zeros(len(flat_1))\n",
    "        # Select cross point\n",
    "        cross_pt = random.randint(0, len(flat_1))\n",
    "        # Swap\n",
    "        child_1[cross_pt:len(flat_1)] = flat_1[cross_pt:len(flat_1)]\n",
    "        child_1[0:cross_pt] = flat_2[0:cross_pt]\n",
    "        child_2[cross_pt:len(flat_1)] = flat_2[cross_pt:len(flat_1)]\n",
    "        child_2[0:cross_pt] = flat_1[0:cross_pt]\n",
    "        # Append to params \n",
    "        params_1.append(nn.Parameter(child_1.reshape(module_shape[i])))\n",
    "        params_2.append(nn.Parameter(child_2.reshape(module_shape[i])))\n",
    "\n",
    "    return Agent(params_1), Agent(params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def evaluate_population(pop):\n",
    "    avg_fit = 0\n",
    "    best_fit = 0\n",
    "    for agent in tqdm(pop):\n",
    "        change_scores(ga_module_list, agent)\n",
    "        fit = test(ga_model.to(device), device)\n",
    "        agent.fitness = fit\n",
    "        avg_fit += fit\n",
    "        if agent.fitness > best_fit:\n",
    "            best_fit = agent.fitness\n",
    "    avg_fit /= len(pop)\n",
    "    \n",
    "    return pop, avg_fit, best_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_generation(pop, size=100, mut_rate=0.01):\n",
    "    new_pop = []\n",
    "    while len(new_pop) < size:\n",
    "        parents = random.choices(pop, k=2, weights=[x.fitness**2 for x in pop])\n",
    "        offspring_ = recombine_agent(parents[0],parents[1])\n",
    "        offspring = [mutation(offspring_[0], mut_rate=mut_rate), mutation(offspring_[1], mut_rate=mut_rate)]\n",
    "        new_pop.extend(offspring) #add offspring to next generation\n",
    "    return new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/nguyen.tung.thanh/anaconda3/envs/fr/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 with pop_fit 0.7981119999999998 | best_fit 0.9572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 with pop_fit 0.815992 | best_fit 0.9597\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12887/1354214288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generation {} with pop_fit {} | best_fit {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpop_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_fit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# record population average fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mnew_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmutation_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12887/3613587929.py\u001b[0m in \u001b[0;36mnext_generation\u001b[0;34m(pop, size, mut_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mparents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moffspring_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecombine_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moffspring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmut_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmut_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnew_pop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add offspring to next generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_pop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12887/126431621.py\u001b[0m in \u001b[0;36mmutation\u001b[0;34m(agent, mut_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mflat_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Get index mutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mreplace_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Mutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_generations = 300\n",
    "population_size = 50\n",
    "\n",
    "pop = init_pop(population_size)\n",
    "\n",
    "mutation_rate = 0.15 # 0.1% mutation rate\n",
    "\n",
    "pop_fit = []\n",
    "\n",
    "pop = init_pop(population_size) # initial population\n",
    "\n",
    "for gen in range(num_generations):\n",
    "    # trainning\n",
    "    pop, avg_fit, best_fit = evaluate_population(pop)\n",
    "    if avg_fit > 0.8:\n",
    "        population_size = 100\n",
    "        mutation_rate = 0.1\n",
    "    if avg_fit > 0.85:\n",
    "        population_size = 150\n",
    "        mutation_rate = 0.05\n",
    "    if avg_fit > 0.9:\n",
    "        population_size = 300\n",
    "        mutation_rate = 0.025\n",
    "    if avg_fit > 0.95:\n",
    "        population_size = 600\n",
    "        mutation_rate = 0.01\n",
    "    print('Generation {} with pop_fit {} | best_fit {}'.format(gen, avg_fit, best_fit))\n",
    "    pop_fit.append(avg_fit) # record population average fitness\n",
    "    new_pop = next_generation(pop, size=population_size, mut_rate=mutation_rate)\n",
    "    pop = new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d026963d5416c1b0f433f3cabaad95ff01da743e2188d1b128ee6199f35f8583"
  },
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
